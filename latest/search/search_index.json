{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Factrainer","text":"<p>Factrainer (Framework Agnostic Cross-validation Trainer) is a machine learning tool that provides a flexible cross-validation training framework. It addresses the limitations of existing cross-validation utilities in popular ML libraries by offering a unified, parallelized approach that retains models and yields out-of-fold (OOF) predictions.</p>"},{"location":"#why-use-factrainer","title":"Why Use Factrainer?","text":"<p>Various ML frameworks (e.g., Scikit-learn, LightGBM) offer cross-validation functions. However, each has different features and interfaces. The table below highlights some widely used cross-validation APIs and which capabilities they support:</p> Framework API OOF prediction return trained models parallel training LightGBM <code>lgb.cv</code> \ud83d\udeab \u2705\ufe0f \ud83d\udeab Scikit-learn <code>GridSearchCV</code> \ud83d\udeab \ud83d\udeab \u2705\ufe0f Scikit-learn <code>cross_val_score</code> \ud83d\udeab \ud83d\udeab \u2705\ufe0f Scikit-learn <code>cross_val_predict</code> \u2705\ufe0f \ud83d\udeab \u2705\ufe0f Scikit-learn <code>cross_validate</code> \ud83d\udeab \u2705\ufe0f \u2705\ufe0f <p>No built-in API combines OOF predictions, trained-model access, and parallelized training\u2014Factrainer does.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Unified Cross-Validation API \u2013 Provides a single, consistent interface to perform K-fold (or any CV) training, acting as a meta-framework that wraps around multiple ML libraries.</li> <li>Parallelized Training \u2013 Run cross-validation folds in parallel to fully utilize multi-core CPUs and speed up model training.</li> <li>Mutable Model Container \u2013 Access each fold\u2019s trained model as a mutable object. This makes it easy to analyze models or create ensembles from the fold models.</li> <li>Out-of-Fold Predictions \u2013 Retrieve out-of-fold predictions for every training instance through a simple API.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install with LightGBM and Scikit-learn support:</p> <pre><code>pip install \"factrainer[lightgbm,sklearn]\"\n</code></pre> <p>To install with all supported backends (LightGBM, Scikit-learn, XGBoost, and CatBoost):</p> <pre><code>pip install \"factrainer[all]\"\n</code></pre> <p>At present, LightGBM and Scikit-learn are the primary supported backends. Support for additional frameworks will be implemented as the project evolves.</p>"},{"location":"#get-started","title":"Get started","text":"<p>Code example: California Housing dataset</p> <pre><code>import lightgbm as lgb\nfrom sklearn.datasets import fetch_california_housing\nfrom factrainer.core import CvModelContainer\nfrom factrainer.lightgbm import LgbDataset, LgbModelConfig, LgbTrainConfig\n\ndata = fetch_california_housing()\ndataset = LgbDataset(\n    dataset=lgb.Dataset(\n        data.data, label=data.target\n    )\n)\nconfig = LgbModelConfig.create(\n    train_config=LgbTrainConfig(\n        params={\"objective\": \"regression\"},\n        callbacks=[lgb.early_stopping(100, verbose=False)],\n    ),\n)\nk_fold = KFold(n_splits=4, shuffle=True, random_state=1)\nmodel = CvModelContainer(config, k_fold)\nmodel.train(dataset, n_jobs=4)\n\n# trained models\nmodel.raw_model\n\n# OOF prediction\ny_pred = model.predict(dataset, n_jobs=4)\nprint(r2_score(data.target, y_pred))\n</code></pre>"},{"location":"#project-status","title":"Project Status","text":"<p>Factrainer is in active development. The goal is to expand support to more frameworks and make the tool even more robust. Contributions, issues, and feedback are welcome to help shape the future of Factrainer.</p>"}]}