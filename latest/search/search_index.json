{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Factrainer","text":"<p>Factrainer (Framework Agnostic Cross-validation Trainer) is a machine learning tool that provides a flexible cross-validation training framework. It addresses the limitations of existing cross-validation utilities in popular ML libraries by offering a unified, parallelized approach that retains models and yields out-of-fold (OOF) predictions.</p>"},{"location":"#why-use-factrainer","title":"Why Use Factrainer?","text":"<p>Various ML frameworks (e.g., Scikit-learn, LightGBM) offer cross-validation functions. However, each has different features and interfaces. The table below highlights some widely used cross-validation APIs and which capabilities they support:</p> Framework API OOF prediction return trained models parallel training LightGBM <code>lgb.cv</code> \ud83d\udeab \u2705\ufe0f \ud83d\udeab Scikit-learn <code>GridSearchCV</code> \ud83d\udeab \ud83d\udeab \u2705\ufe0f Scikit-learn <code>cross_val_score</code> \ud83d\udeab \ud83d\udeab \u2705\ufe0f Scikit-learn <code>cross_val_predict</code> \u2705\ufe0f \ud83d\udeab \u2705\ufe0f Scikit-learn <code>cross_validate</code> \ud83d\udeab \u2705\ufe0f \u2705\ufe0f <p>No built-in API combines OOF predictions, trained-model access, and parallelized training\u2014Factrainer does.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Unified Cross-Validation API \u2013 Provides a single, consistent interface to perform K-fold (or any CV) training, acting as a meta-framework that wraps around multiple ML libraries.</li> <li>Parallelized Training \u2013 Run cross-validation folds in parallel to fully utilize multi-core CPUs and speed up model training.</li> <li>Mutable Model Container \u2013 Access each fold\u2019s trained model as a mutable object. This makes it easy to analyze models or create ensembles from the fold models.</li> <li>Out-of-Fold Predictions \u2013 Retrieve out-of-fold predictions for every training instance through a simple API.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install with LightGBM and Scikit-learn support:</p> <pre><code>pip install \"factrainer[lightgbm,sklearn]\"\n</code></pre> <p>To install with all supported backends (LightGBM, Scikit-learn, XGBoost, and CatBoost):</p> <pre><code>pip install \"factrainer[all]\"\n</code></pre> <p>At present, LightGBM and Scikit-learn are the primary supported backends. Support for additional frameworks will be implemented as the project evolves.</p>"},{"location":"#get-started","title":"Get started","text":"<p>Code example: California Housing dataset</p> <pre><code>import lightgbm as lgb\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import KFold\nfrom factrainer.core import CvModelContainer, EvalMode\nfrom factrainer.lightgbm import LgbDataset, LgbModelConfig, LgbTrainConfig\n\ndata = fetch_california_housing()\ndataset = LgbDataset(\n    dataset=lgb.Dataset(\n        data.data, label=data.target\n    )\n)\nconfig = LgbModelConfig.create(\n    train_config=LgbTrainConfig(\n        params={\"objective\": \"regression\", \"verbose\": -1},\n        callbacks=[lgb.early_stopping(100, verbose=False)],\n    ),\n)\nk_fold = KFold(n_splits=4, shuffle=True, random_state=1)\nmodel = CvModelContainer(config, k_fold)\nmodel.train(dataset, n_jobs=4)\n\n# Get OOF predictions\ny_pred = model.predict(dataset, n_jobs=4)\n\n# Evaluate predictions\nmetric = model.evaluate(data.target, y_pred, r2_score)\nprint(f\"R2 Score: {metric:.4f}\")\n\n# Or get per-fold metrics\nmetrics = model.evaluate(\n    data.target, y_pred, r2_score, eval_mode=EvalMode.FOLD_WISE\n)\nprint(f\"R2 Scores by fold: {[f'{m:.4f}' for m in metrics]}\")\n\n# Access trained models\nmodel.raw_model\n</code></pre>"},{"location":"#project-status","title":"Project Status","text":"<p>Factrainer is in active development. The goal is to expand support to more frameworks and make the tool even more robust. Contributions, issues, and feedback are welcome to help shape the future of Factrainer.</p>"},{"location":"_tutorial/","title":"Tutorial","text":"<p>This tutorial will guide you through the basics of using Factrainer for cross-validation in machine learning tasks.</p>"},{"location":"_tutorial/#installation","title":"Installation","text":"<p>First, install Factrainer with the desired backends:</p> <pre><code># Install with LightGBM and scikit-learn support\npip install \"factrainer[lightgbm,sklearn]\"\n\n# Or install with all supported backends\npip install \"factrainer[all]\"\n</code></pre>"},{"location":"_tutorial/#basic-concepts","title":"Basic Concepts","text":"<p>Factrainer is designed around a few key concepts:</p> <ol> <li>Datasets: Wrappers for data that can be used with different ML frameworks</li> <li>Model Configurations: Configuration objects that define how models are trained and used for prediction</li> <li>Model Containers: Objects that hold trained models and provide methods for training and prediction</li> </ol>"},{"location":"_tutorial/#quick-start","title":"Quick Start","text":"<p>Let's start with a simple example using LightGBM for regression:</p> <pre><code>import lightgbm as lgb\nimport numpy as np\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import KFold\nfrom factrainer.core import CvModelContainer\nfrom factrainer.lightgbm import LgbDataset, LgbModelConfig, LgbTrainConfig\n\n# Load data\ndata = fetch_california_housing()\nX, y = data.data, data.target\n\n# Create dataset\ndataset = LgbDataset(\n    dataset=lgb.Dataset(X, label=y)\n)\n\n# Configure model\nconfig = LgbModelConfig.create(\n    train_config=LgbTrainConfig(\n        params={\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.1,\n            \"num_leaves\": 31,\n            \"verbose\": -1\n        },\n        num_boost_round=100,\n        callbacks=[lgb.early_stopping(10, verbose=False)],\n    ),\n)\n\n# Set up cross-validation\nk_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Create and train model\nmodel = CvModelContainer(config, k_fold)\nmodel.train(dataset, n_jobs=4)\n\n# Get OOF predictions\ny_pred = model.predict(dataset, n_jobs=4)\nprint(f\"R\u00b2 score: {r2_score(y, y_pred):.4f}\")\n</code></pre>"},{"location":"_tutorial/#working-with-different-frameworks","title":"Working with Different Frameworks","text":"<p>Factrainer provides a unified API for working with different ML frameworks. Here's how to use it with scikit-learn:</p> <pre><code>import numpy as np\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom factrainer.core import CvModelContainer\nfrom factrainer.sklearn import SklearnDataset, SklearnModelConfig, SklearnTrainConfig\n\n# Load data\ndata = fetch_california_housing()\nX, y = data.data, data.target\n\n# Create dataset\ndataset = SklearnDataset(X=X, y=y)\n\n# Create pipeline with preprocessing and model\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n])\n\n# Configure model\nconfig = SklearnModelConfig.create(\n    train_config=SklearnTrainConfig(\n        estimator=pipeline,\n        fit_params={\n            'model__n_jobs': -1\n        }\n    ),\n)\n\n# Set up cross-validation\nk_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Create and train model\nmodel = CvModelContainer(config, k_fold)\nmodel.train(dataset, n_jobs=4)\n\n# Get OOF predictions\ny_pred = model.predict(dataset, n_jobs=4)\nprint(f\"R\u00b2 score: {r2_score(y, y_pred):.4f}\")\n</code></pre>"},{"location":"_tutorial/#advanced-features","title":"Advanced Features","text":""},{"location":"_tutorial/#out-of-fold-predictions-vs-ensemble-predictions","title":"Out-of-Fold Predictions vs. Ensemble Predictions","text":"<p>Factrainer supports two prediction modes:</p> <ol> <li>Out-of-Fold (OOF) Predictions: Predictions for the training data using models trained on other folds</li> <li>Ensemble Predictions: Predictions using an ensemble of all trained models</li> </ol> <pre><code># Get OOF predictions (default)\ny_pred_oof = model.predict(dataset, n_jobs=4)\n\n# Get ensemble predictions\nfrom factrainer.core.cv.config import PredMode\ny_pred_ensemble = model.predict(dataset, n_jobs=4, mode=PredMode.AVG_ENSEMBLE)\n</code></pre>"},{"location":"_tutorial/#accessing-trained-models","title":"Accessing Trained Models","text":"<p>You can access the trained models directly:</p> <pre><code># Get the raw models\nraw_models = model.raw_model\n\n# Access individual models\nfor i, raw_model in enumerate(raw_models.models):\n    print(f\"Model {i}: {raw_model}\")\n</code></pre>"},{"location":"_tutorial/#customizing-training-and-prediction","title":"Customizing Training and Prediction","text":"<p>You can customize the training and prediction process by modifying the configuration:</p> <pre><code># Change the training configuration\nmodel.train_config = LgbTrainConfig(\n    params={\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.05,  # Changed from 0.1\n        \"num_leaves\": 63,       # Changed from 31\n        \"verbose\": -1\n    },\n    num_boost_round=200,        # Changed from 100\n    callbacks=[lgb.early_stopping(20, verbose=False)],  # Changed from 10\n)\n\n# Change the prediction configuration\nfrom factrainer.lightgbm import LgbPredictConfig\nmodel.pred_config = LgbPredictConfig(\n    num_iteration=100\n)\n</code></pre>"},{"location":"_tutorial/#working-with-different-data-types","title":"Working with Different Data Types","text":"<p>Factrainer supports various data types through its dataset wrappers:</p>"},{"location":"_tutorial/#lightgbm","title":"LightGBM","text":"<pre><code>import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nfrom factrainer.lightgbm import LgbDataset\n\n# NumPy array\nX_np = np.array(...)\ny_np = np.array(...)\ndataset_np = LgbDataset(\n    dataset=lgb.Dataset(X_np, label=y_np)\n)\n\n# Pandas DataFrame\nX_pd = pd.DataFrame(...)\ny_pd = pd.Series(...)\ndataset_pd = LgbDataset(\n    dataset=lgb.Dataset(X_pd, label=y_pd)\n)\n\n# With additional parameters\ndataset = LgbDataset(\n    dataset=lgb.Dataset(\n        X,\n        label=y,\n        weight=sample_weights,\n        group=group_info,\n        init_score=init_scores,\n        feature_name=feature_names,\n        categorical_feature=categorical_features,\n    )\n)\n</code></pre>"},{"location":"_tutorial/#scikit-learn","title":"scikit-learn","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom factrainer.sklearn import SklearnDataset\n\n# NumPy array\nX_np = np.array(...)\ny_np = np.array(...)\ndataset_np = SklearnDataset(X=X_np, y=y_np)\n\n# Pandas DataFrame\nX_pd = pd.DataFrame(...)\ny_pd = pd.Series(...)\ndataset_pd = SklearnDataset(X=X_pd, y=y_pd)\n\n# With sample weights\ndataset = SklearnDataset(X=X, y=y, sample_weight=sample_weights)\n</code></pre>"},{"location":"_tutorial/#next-steps","title":"Next Steps","text":"<p>Now that you've learned the basics of Factrainer, you can:</p> <ul> <li>Check out the API Reference for detailed documentation of all classes and methods</li> <li>Explore the Examples for more advanced usage patterns</li> <li>Learn about the Plugin Architecture if you want to extend Factrainer with your own ML framework</li> </ul>"},{"location":"tutorial/","title":"Tutorial","text":"<p>WIP</p>"},{"location":"examples/","title":"Examples","text":"<p>This section provides practical examples of using Factrainer for various machine learning tasks. These examples demonstrate how to use Factrainer with different frameworks and for different types of problems.</p>"},{"location":"examples/#classification-examples","title":"Classification Examples","text":"<ul> <li>Binary Classification: Examples of binary classification tasks using Factrainer</li> <li>Multi-class Classification: Examples of multi-class classification tasks using Factrainer</li> </ul>"},{"location":"examples/#regression-examples","title":"Regression Examples","text":"<ul> <li>Simple Regression: Examples of simple regression tasks using Factrainer</li> <li>Multiple Regression: Examples of multiple regression tasks using Factrainer</li> </ul>"},{"location":"examples/#framework-specific-examples","title":"Framework-Specific Examples","text":"<ul> <li>LightGBM Examples: Examples of using Factrainer with LightGBM</li> <li>scikit-learn Examples: Examples of using Factrainer with scikit-learn</li> </ul>"},{"location":"examples/#advanced-examples","title":"Advanced Examples","text":"<ul> <li>Custom Metrics: Examples of using custom metrics with Factrainer</li> <li>Feature Engineering: Examples of feature engineering with Factrainer</li> <li>Hyperparameter Tuning: Examples of hyperparameter tuning with Factrainer</li> </ul>"},{"location":"examples/classification/","title":"Classification Examples","text":"<p>This page provides examples of using Factrainer for classification tasks.</p>"},{"location":"examples/classification/#binary-classification","title":"Binary Classification","text":""},{"location":"examples/classification/#breast-cancer-classification-with-lightgbm","title":"Breast Cancer Classification with LightGBM","text":"<p>This example demonstrates how to use Factrainer with LightGBM for binary classification on the Breast Cancer Wisconsin dataset.</p> <pre><code>import lightgbm as lgb\nimport numpy as np\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.model_selection import KFold, train_test_split\nfrom factrainer.core import CvModelContainer\nfrom factrainer.lightgbm import LgbDataset, LgbModelConfig, LgbTrainConfig\n\n# Load data\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create dataset\ntrain_dataset = LgbDataset(\n    dataset=lgb.Dataset(X_train, label=y_train)\n)\ntest_dataset = LgbDataset(\n    dataset=lgb.Dataset(X_test, label=y_test)\n)\n\n# Configure model\nconfig = LgbModelConfig.create(\n    train_config=LgbTrainConfig(\n        params={\n            \"objective\": \"binary\",\n            \"metric\": \"binary_logloss\",\n            \"learning_rate\": 0.1,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 5,\n            \"verbose\": -1\n        },\n        num_boost_round=100,\n        callbacks=[lgb.early_stopping(10, verbose=False)],\n    ),\n)\n\n# Set up cross-validation\nk_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Create and train model\nmodel = CvModelContainer(config, k_fold)\nmodel.train(train_dataset, n_jobs=4)\n\n# Get OOF predictions\ny_pred_oof = model.predict(train_dataset, n_jobs=4)\nprint(f\"OOF ROC AUC: {roc_auc_score(y_train, y_pred_oof):.4f}\")\nprint(f\"OOF Accuracy: {accuracy_score(y_train, y_pred_oof &gt; 0.5):.4f}\")\n\n# Get test predictions\ny_pred_test = model.predict(test_dataset, n_jobs=4, mode=\"AVG_ENSEMBLE\")\nprint(f\"Test ROC AUC: {roc_auc_score(y_test, y_pred_test):.4f}\")\nprint(f\"Test Accuracy: {accuracy_score(y_test, y_pred_test &gt; 0.5):.4f}\")\n</code></pre>"},{"location":"examples/classification/#ionosphere-classification-with-scikit-learn","title":"Ionosphere Classification with scikit-learn","text":"<p>This example demonstrates how to use Factrainer with scikit-learn for binary classification on the Ionosphere dataset.</p> <pre><code>import numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom factrainer.core import CvModelContainer\nfrom factrainer.sklearn import SklearnDataset, SklearnModelConfig, SklearnTrainConfig\n\n# Load data\nX, y = fetch_openml(\"ionosphere\", return_X_y=True, as_frame=False)\ny = (y == \"g\").astype(int)  # Convert to binary (0/1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create datasets\ntrain_dataset = SklearnDataset(X=X_train, y=y_train)\ntest_dataset = SklearnDataset(X=X_test, y=y_test)\n\n# Create pipeline with preprocessing and model\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', RandomForestClassifier(n_estimators=100, random_state=42))\n])\n\n# Configure model\nconfig = SklearnModelConfig.create(\n    train_config=SklearnTrainConfig(\n        estimator=pipeline,\n        fit_params={\n            'model__n_jobs': -1\n        }\n    ),\n)\n\n# Set up cross-validation\nk_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Create and train model\nmodel = CvModelContainer(config, k_fold)\nmodel.train(train_dataset, n_jobs=4)\n\n# Get OOF predictions\ny_pred_oof = model.predict(train_dataset, n_jobs=4)\nprint(f\"OOF ROC AUC: {roc_auc_score(y_train, y_pred_oof):.4f}\")\nprint(f\"OOF Accuracy: {accuracy_score(y_train, y_pred_oof &gt; 0.5):.4f}\")\n\n# Get test predictions\ny_pred_test = model.predict(test_dataset, n_jobs=4, mode=\"AVG_ENSEMBLE\")\nprint(f\"Test ROC AUC: {roc_auc_score(y_test, y_pred_test):.4f}\")\nprint(f\"Test Accuracy: {accuracy_score(y_test, y_pred_test &gt; 0.5):.4f}\")\n</code></pre>"},{"location":"examples/classification/#multi-class-classification","title":"Multi-class Classification","text":""},{"location":"examples/classification/#iris-classification-with-lightgbm","title":"Iris Classification with LightGBM","text":"<p>This example demonstrates how to use Factrainer with LightGBM for multi-class classification on the Iris dataset.</p> <pre><code>import lightgbm as lgb\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold, train_test_split\nfrom factrainer.core import CvModelContainer\nfrom factrainer.lightgbm import LgbDataset, LgbModelConfig, LgbTrainConfig\n\n# Load data\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create dataset\ntrain_dataset = LgbDataset(\n    dataset=lgb.Dataset(X_train, label=y_train)\n)\ntest_dataset = LgbDataset(\n    dataset=lgb.Dataset(X_test, label=y_test)\n)\n\n# Configure model\nconfig = LgbModelConfig.create(\n    train_config=LgbTrainConfig(\n        params={\n            \"objective\": \"multiclass\",\n            \"num_class\": 3,\n            \"metric\": \"multi_logloss\",\n            \"learning_rate\": 0.1,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 5,\n            \"verbose\": -1\n        },\n        num_boost_round=100,\n        callbacks=[lgb.early_stopping(10, verbose=False)],\n    ),\n)\n\n# Set up cross-validation\nk_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Create and train model\nmodel = CvModelContainer(config, k_fold)\nmodel.train(train_dataset, n_jobs=4)\n\n# Get OOF predictions\ny_pred_oof = model.predict(train_dataset, n_jobs=4)\ny_pred_oof_class = np.argmax(y_pred_oof, axis=1)\nprint(f\"OOF Accuracy: {accuracy_score(y_train, y_pred_oof_class):.4f}\")\n\n# Get test predictions\ny_pred_test = model.predict(test_dataset, n_jobs=4, mode=\"AVG_ENSEMBLE\")\ny_pred_test_class = np.argmax(y_pred_test, axis=1)\nprint(f\"Test Accuracy: {accuracy_score(y_test, y_pred_test_class):.4f}\")\n</code></pre>"},{"location":"examples/classification/#lightgbm-examples","title":"LightGBM Examples","text":""},{"location":"examples/classification/#custom-metrics","title":"Custom Metrics","text":"<p>This example demonstrates how to use custom metrics with LightGBM in Factrainer.</p> <pre><code>import lightgbm as lgb\nimport numpy as np\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import KFold, train_test_split\nfrom factrainer.core import CvModelContainer\nfrom factrainer.lightgbm import LgbDataset, LgbModelConfig, LgbTrainConfig\n\n# Load data\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create dataset\ntrain_dataset = LgbDataset(\n    dataset=lgb.Dataset(X_train, label=y_train)\n)\ntest_dataset = LgbDataset(\n    dataset=lgb.Dataset(X_test, label=y_test)\n)\n\n# Define custom metric\ndef custom_accuracy(preds, train_data):\n    labels = train_data.get_label()\n    preds = preds &gt; 0.5\n    return 'custom_accuracy', np.mean(labels == preds), True\n\n# Configure model\nconfig = LgbModelConfig.create(\n    train_config=LgbTrainConfig(\n        params={\n            \"objective\": \"binary\",\n            \"learning_rate\": 0.1,\n            \"num_leaves\": 31,\n            \"verbose\": -1\n        },\n        num_boost_round=100,\n        feval=custom_accuracy,\n        callbacks=[lgb.early_stopping(10, verbose=False)],\n    ),\n)\n\n# Set up cross-validation\nk_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Create and train model\nmodel = CvModelContainer(config, k_fold)\nmodel.train(train_dataset, n_jobs=4)\n\n# Get OOF predictions\ny_pred_oof = model.predict(train_dataset, n_jobs=4)\n\n# Get test predictions\ny_pred_test = model.predict(test_dataset, n_jobs=4, mode=\"AVG_ENSEMBLE\")\n</code></pre>"},{"location":"examples/classification/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>This example demonstrates how to use Factrainer with hyperparameter tuning.</p> <p>```python import lightgbm as lgb import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.metrics import roc_auc_score from sklearn.model_selection import KFold, train_test_split from factrainer.core import CvModelContainer from factrainer.lightgbm import LgbDataset, LgbModelConfig, LgbTrainConfig</p>"},{"location":"examples/classification/#load-data","title":"Load data","text":"<p>X, y = load_breast_cancer(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</p>"},{"location":"examples/classification/#create-dataset","title":"Create dataset","text":"<p>train_dataset = LgbDataset(     dataset=lgb.Dataset(X_train, label=y_train) ) test_dataset = LgbDataset(     dataset=lgb.Dataset(X_test, label=y_test) )</p>"},{"location":"examples/classification/#set-up-cross-validation","title":"Set up cross-validation","text":"<p>k_fold = KFold(n_splits=5, shuffle=True, random_state=42)</p>"},{"location":"examples/classification/#define-hyperparameter-grid","title":"Define hyperparameter grid","text":"<p>param_grid = [     {\"learning_rate\": 0.01, \"num_leaves\": 31},     {\"learning_rate\": 0.05, \"num_leaves\": 31},     {\"learning_rate\": 0.1, \"num_leaves\": 31},     {\"learning_rate\": 0.01, \"num_leaves\": 63},     {\"learning_rate\": 0.05, \"num_leaves\": 63},     {\"learning_rate\": 0.1, \"num_leaves\": 63}, ]</p>"},{"location":"examples/classification/#train-models-with-different-hyperparameters","title":"Train models with different hyperparameters","text":"<p>results = [] for params in param_grid:     # Configure model     config = LgbModelConfig.create(         train_config=LgbTrainConfig(             params={                 \"objective\": \"binary\",                 \"metric\": \"binary_logloss\",                 \"learning_rate\": params[\"learning_rate\"],                 \"num_leaves\": params[\"num_leaves\"],                 \"feature_fraction\": 0.8,                 \"bagging_fraction\": 0.8,                 \"bagging_freq\": 5,                 \"verbose\": -1             },             num_boost_round=100,             callbacks=[lgb.early_stopping(10, verbose=False)],         ),     )</p> <pre><code># Create and train model\nmodel = CvModelContainer(config, k_fold)\nmodel.train(train_dataset, n_jobs=4)\n\n# Get OOF predictions\ny_pred_oof = model.predict(train_dataset, n_jobs=4)\noof_auc = roc_auc_score(y_train, y_pred_oof)\n\n# Get test predictions\ny_pred_test = model.predict(test_dataset, n_jobs=4, mode=\"AVG_ENSEMBLE\")\ntest_auc = roc_auc_score(y_test, y_pred_test)\n\nresults.append({\n    \"params\": params,\n    \"oof_auc\": oof_auc,\n    \"test_auc\": test_auc\n})\n</code></pre>"},{"location":"examples/classification/#find-best-model","title":"Find best model","text":"<p>best_model = max(results, key=lambda x: x[\"oof_auc\"]) print(f\"Best model: {best_model['params']}\") print(f\"OOF AUC: {best_model['oof_auc']:.4f}\") print(f\"Test AUC: {best_model['test_auc']:.4f}\")</p>"},{"location":"examples/regression/","title":"Regression Examples","text":"<p>This page provides examples of using Factrainer for regression tasks.</p>"},{"location":"examples/regression/#simple-regression","title":"Simple Regression","text":""},{"location":"examples/regression/#california-housing-regression-with-lightgbm","title":"California Housing Regression with LightGBM","text":"<p>This example demonstrates how to use Factrainer with LightGBM for regression on the California Housing dataset.</p> <pre><code>import lightgbm as lgb\nimport numpy as np\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import KFold, train_test_split\nfrom factrainer.core import CvModelContainer\nfrom factrainer.lightgbm import LgbDataset, LgbModelConfig, LgbTrainConfig\n\n# Load data\ndata = fetch_california_housing()\nX, y = data.data, data.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create dataset\ntrain_dataset = LgbDataset(\n    dataset=lgb.Dataset(X_train, label=y_train)\n)\ntest_dataset = LgbDataset(\n    dataset=lgb.Dataset(X_test, label=y_test)\n)\n\n# Configure model\nconfig = LgbModelConfig.create(\n    train_config=LgbTrainConfig(\n        params={\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.1,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 5,\n            \"verbose\": -1\n        },\n        num_boost_round=100,\n        callbacks=[lgb.early_stopping(10, verbose=False)],\n    ),\n)\n\n# Set up cross-validation\nk_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Create and train model\nmodel = CvModelContainer(config, k_fold)\nmodel.train(train_dataset, n_jobs=4)\n\n# Get OOF predictions\ny_pred_oof = model.predict(train_dataset, n_jobs=4)\nprint(f\"OOF RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_oof)):.4f}\")\nprint(f\"OOF R\u00b2: {r2_score(y_train, y_pred_oof):.4f}\")\n\n# Get test predictions\ny_pred_test = model.predict(test_dataset, n_jobs=4, mode=\"AVG_ENSEMBLE\")\nprint(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.4f}\")\nprint(f\"Test R\u00b2: {r2_score(y_test, y_pred_test):.4f}\")\n</code></pre>"},{"location":"examples/regression/#diabetes-regression-with-scikit-learn","title":"Diabetes Regression with scikit-learn","text":"<p>This example demonstrates how to use Factrainer with scikit-learn for regression on the Diabetes dataset.</p> <pre><code>import numpy as np\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom factrainer.core import CvModelContainer\nfrom factrainer.sklearn import SklearnDataset, SklearnModelConfig, SklearnTrainConfig\n\n# Load data\nX, y = load_diabetes(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create datasets\ntrain_dataset = SklearnDataset(X=X_train, y=y_train)\ntest_dataset = SklearnDataset(X=X_test, y=y_test)\n\n# Create pipeline with preprocessing and model\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n])\n\n# Configure model\nconfig = SklearnModelConfig.create(\n    train_config=SklearnTrainConfig(\n        estimator=pipeline,\n        fit_params={\n            'model__n_jobs': -1\n        }\n    ),\n)\n\n# Set up cross-validation\nk_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Create and train model\nmodel = CvModelContainer(config, k_fold)\nmodel.train(train_dataset, n_jobs=4)\n\n# Get OOF predictions\ny_pred_oof = model.predict(train_dataset, n_jobs=4)\nprint(f\"OOF RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_oof)):.4f}\")\nprint(f\"OOF R\u00b2: {r2_score(y_train, y_pred_oof):.4f}\")\n\n# Get test predictions\ny_pred_test = model.predict(test_dataset, n_jobs=4, mode=\"AVG_ENSEMBLE\")\nprint(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.4f}\")\nprint(f\"Test R\u00b2: {r2_score(y_test, y_pred_test):.4f}\")\n</code></pre>"},{"location":"examples/regression/#multiple-regression","title":"Multiple Regression","text":""},{"location":"examples/regression/#boston-housing-regression-with-lightgbm","title":"Boston Housing Regression with LightGBM","text":"<p>This example demonstrates how to use Factrainer with LightGBM for regression on the Boston Housing dataset.</p> <pre><code>import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import KFold, train_test_split\nfrom factrainer.core import CvModelContainer\nfrom factrainer.lightgbm import LgbDataset, LgbModelConfig, LgbTrainConfig\n\n# Load data\nboston = fetch_openml(name=\"boston\", version=1, as_frame=True)\nX, y = boston.data, boston.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create dataset\ntrain_dataset = LgbDataset(\n    dataset=lgb.Dataset(X_train, label=y_train)\n)\ntest_dataset = LgbDataset(\n    dataset=lgb.Dataset(X_test, label=y_test)\n)\n\n# Configure model\nconfig = LgbModelConfig.create(\n    train_config=LgbTrainConfig(\n        params={\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 5,\n            \"verbose\": -1\n        },\n        num_boost_round=200,\n        callbacks=[lgb.early_stopping(20, verbose=False)],\n    ),\n)\n\n# Set up cross-validation\nk_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Create and train model\nmodel = CvModelContainer(config, k_fold)\nmodel.train(train_dataset, n_jobs=4)\n\n# Get OOF predictions\ny_pred_oof = model.predict(train_dataset, n_jobs=4)\nprint(f\"OOF RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_oof)):.4f}\")\nprint(f\"OOF R\u00b2: {r2_score(y_train, y_pred_oof):.4f}\")\n\n# Get test predictions\ny_pred_test = model.predict(test_dataset, n_jobs=4, mode=\"AVG_ENSEMBLE\")\nprint(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.4f}\")\nprint(f\"Test R\u00b2: {r2_score(y_test, y_pred_test):.4f}\")\n\n# Feature importance\nfeature_importance = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': model.raw_model.models[0].model.feature_importance()\n}).sort_values('Importance', ascending=False)\nprint(feature_importance)\n</code></pre>"},{"location":"examples/regression/#scikit-learn-examples","title":"scikit-learn Examples","text":""},{"location":"examples/regression/#feature-engineering","title":"Feature Engineering","text":"<p>This example demonstrates how to use Factrainer with scikit-learn for feature engineering in regression tasks.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler\nfrom factrainer.core import CvModelContainer\nfrom factrainer.sklearn import SklearnDataset, SklearnModelConfig, SklearnTrainConfig\n\n# Load data\ndata = fetch_california_housing()\nX, y = data.data, data.target\n\n# Convert to DataFrame for better feature handling\nX_df = pd.DataFrame(X, columns=data.feature_names)\n\n# Add some categorical features for demonstration\nX_df['MedInc_Cat'] = pd.qcut(X_df['MedInc'], 4, labels=['Low', 'Medium', 'High', 'Very High'])\nX_df['AveOccup_Cat'] = pd.qcut(X_df['AveOccup'], 3, labels=['Low', 'Medium', 'High'])\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.2, random_state=42)\n\n# Define feature types\nnumeric_features = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\ncategorical_features = ['MedInc_Cat', 'AveOccup_Cat']\n\n# Create preprocessing pipeline\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Create full pipeline\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', GradientBoostingRegressor(n_estimators=100, random_state=42))\n])\n\n# Create datasets\ntrain_dataset = SklearnDataset(X=X_train, y=y_train)\ntest_dataset = SklearnDataset(X=X_test, y=y_test)\n\n# Configure model\nconfig = SklearnModelConfig.create(\n    train_config=SklearnTrainConfig(\n        estimator=pipeline\n    ),\n)\n\n# Set up cross-validation\nk_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Create and train model\nmodel = CvModelContainer(config, k_fold)\nmodel.train(train_dataset, n_jobs=4)\n\n# Get OOF predictions\ny_pred_oof = model.predict(train_dataset, n_jobs=4)\nprint(f\"OOF RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_oof)):.4f}\")\nprint(f\"OOF R\u00b2: {r2_score(y_train, y_pred_oof):.4f}\")\n\n# Get test predictions\ny_pred_test = model.predict(test_dataset, n_jobs=4, mode=\"AVG_ENSEMBLE\")\nprint(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.4f}\")\nprint(f\"Test R\u00b2: {r2_score(y_test, y_pred_test):.4f}\")\n</code></pre>"},{"location":"examples/regression/#stacking-regression","title":"Stacking Regression","text":"<p>This example demonstrates how to use Factrainer with scikit-learn for stacking regression.</p> <p>```python import numpy as np from sklearn.datasets import fetch_california_housing from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor from sklearn.linear_model import ElasticNet, Ridge from sklearn.metrics import mean_squared_error, r2_score from sklearn.model_selection import KFold, train_test_split from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline from factrainer.core import CvModelContainer from factrainer.sklearn import SklearnDataset, SklearnModelConfig, SklearnTrainConfig</p>"},{"location":"examples/regression/#load-data","title":"Load data","text":"<p>data = fetch_california_housing() X, y = data.data, data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</p>"},{"location":"examples/regression/#create-datasets","title":"Create datasets","text":"<p>train_dataset = SklearnDataset(X=X_train, y=y_train) test_dataset = SklearnDataset(X=X_test, y=y_test)</p>"},{"location":"examples/regression/#set-up-cross-validation","title":"Set up cross-validation","text":"<p>k_fold = KFold(n_splits=5, shuffle=True, random_state=42)</p>"},{"location":"examples/regression/#train-base-models-and-get-oof-predictions","title":"Train base models and get OOF predictions","text":"<p>base_models = [     ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),     ('gbm', GradientBoostingRegressor(n_estimators=100, random_state=42)),     ('ridge', Ridge(alpha=1.0, random_state=42)),     ('elasticnet', ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42)) ]</p> <p>oof_predictions = np.zeros((X_train.shape[0], len(base_models))) test_predictions = np.zeros((X_test.shape[0], len(base_models)))</p> <p>for i, (name, estimator) in enumerate(base_models):     # Create pipeline     pipeline = Pipeline([         ('scaler', StandardScaler()),         ('model', estimator)     ])</p> <pre><code># Configure model\nconfig = SklearnModelConfig.create(\n    train_config=SklearnTrainConfig(\n        estimator=pipeline\n    ),\n)\n\n# Create and train model\nmodel = CvModelContainer(config, k_fold)\nmodel.train(train_dataset, n_jobs=4)\n\n# Get OOF predictions\noof_predictions[:, i] = model.predict(train_dataset, n_jobs=4)\n\n# Get test predictions\ntest_predictions[:, i] = model.predict(test_dataset, n_jobs=4, mode=\"AVG_ENSEMBLE\")\n\n# Print model performance\nprint(f\"{name} - OOF RMSE: {np.sqrt(mean_squared_error(y_train, oof_predictions[:, i])):.4f}\")\nprint(f\"{name} - OOF R\u00b2: {r2_score(y_train, oof_predictions[:, i]):.4f}\")\n</code></pre>"},{"location":"examples/regression/#create-meta-model-dataset","title":"Create meta-model dataset","text":"<p>meta_train_dataset = SklearnDataset(X=oof_predictions, y=y_train) meta_test_dataset = SklearnDataset(X=test_predictions, y=y_test)</p>"},{"location":"examples/regression/#configure-meta-model","title":"Configure meta-model","text":"<p>meta_config = SklearnModelConfig.create(     train_config=SklearnTrainConfig(         estimator=Ridge(alpha=1.0, random_state=42)     ), )</p>"},{"location":"examples/regression/#create-and-train-meta-model","title":"Create and train meta-model","text":"<p>meta_model = CvModelContainer(meta_config, k_fold) meta_model.train(meta_train_dataset, n_jobs=4)</p>"},{"location":"examples/regression/#get-meta-model-predictions","title":"Get meta-model predictions","text":"<p>meta_pred_oof = meta_model.predict(meta_train_dataset, n_jobs=4) meta_pred_test = meta_model.predict(meta_test_dataset, n_jobs=4, mode=\"AVG_ENSEMBLE\")</p>"},{"location":"examples/regression/#print-meta-model-performance","title":"Print meta-model performance","text":"<p>print(f\"Meta-model - OOF RMSE: {np.sqrt(mean_squared_error(y_train, meta_pred_oof)):.4f}\") print(f\"Meta-model - OOF R\u00b2: {r2_score(y_train, meta_pred_oof):.4f}\") print(f\"Meta-model - Test RMSE: {np.sqrt(mean_squared_error(y_test, meta_pred_test)):.4f}\") print(f\"Meta-model - Test R\u00b2: {r2_score(y_test, meta_pred_test):.4f}\")</p>"},{"location":"reference/","title":"API Reference","text":"<p>This section provides detailed documentation for all public modules, classes, and functions in the Factrainer library.</p>"},{"location":"reference/#public-api","title":"Public API","text":"<p>Factrainer consists of the following sub-packages:</p> <ul> <li>factrainer.core: Main cross-validation functionality that is installed by default. Used in combination with the following framework-specific plugin packages.</li> <li>factrainer.lightgbm: Optionally installable plugin package for LightGBM integration.</li> <li>factrainer.sklearn: Optionally installable plugin package for Scikit-learn integration.</li> <li>factrainer.xgboost: Optionally installable plugin package for XGBoost integration (planned for future release).</li> <li>factrainer.catboost: Optionally installable plugin package for CatBoost integration (planned for future release).</li> </ul>"},{"location":"reference/catboost/","title":"factrainer.catboost","text":"<p>Implementation Status</p> <p>The CatBoost integration is currently WIP. It will be added in a future release.</p>"},{"location":"reference/core/","title":"factrainer.core","text":"<p>Main cross-validation functionality that is installed by default. Used in combination with the framework-specific plugin packages.</p>"},{"location":"reference/core/cvmodelcontainer/","title":"CvModelContainer","text":""},{"location":"reference/core/cvmodelcontainer/#factrainer.core.CvModelContainer","title":"CvModelContainer","text":"<pre><code>CvModelContainer(\n    model_config: BaseMlModelConfig[T, U, V, W],\n    k_fold: _BaseKFold | SplittedDatasetsIndices,\n)\n</code></pre> <p>Cross-validation model container for machine learning models.</p> <p>This class provides a container for cross-validation models. It takes a model configuration and a cross-validation splitter, and provides methods for training models and making predictions.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>BaseMlModelConfig</code> <p>The model configuration, which includes the learner, predictor, training configuration, and prediction configuration.</p> required <code>k_fold</code> <code>_BaseKFold or SplittedDatasetsIndices</code> <p>The cross-validation splitter, which can be either a scikit-learn _BaseKFold object or a SplittedDatasetsIndices object. If _BaseKFold is specified, the same indices will be used for both validation and test data. To specify custom indices without such constraints, use SplittedDatasetsIndices.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lightgbm as lgb\n&gt;&gt;&gt; from sklearn.datasets import fetch_california_housing\n&gt;&gt;&gt; from sklearn.metrics import r2_score\n&gt;&gt;&gt; from sklearn.model_selection import KFold\n&gt;&gt;&gt; from factrainer.core import CvModelContainer, EvalMode\n&gt;&gt;&gt; from factrainer.lightgbm import LgbDataset, LgbModelConfig, LgbTrainConfig\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load data\n&gt;&gt;&gt; data = fetch_california_housing()\n&gt;&gt;&gt; dataset = LgbDataset(dataset=lgb.Dataset(data.data, label=data.target))\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Configure model\n&gt;&gt;&gt; config = LgbModelConfig.create(\n...     train_config=LgbTrainConfig(\n...         params={\"objective\": \"regression\", \"verbose\": -1},\n...         callbacks=[lgb.early_stopping(100, verbose=False)],\n...     ),\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Set up cross-validation\n&gt;&gt;&gt; k_fold = KFold(n_splits=4, shuffle=True, random_state=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create and train model\n&gt;&gt;&gt; model = CvModelContainer(config, k_fold)\n&gt;&gt;&gt; model.train(dataset, n_jobs=4)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get OOF predictions\n&gt;&gt;&gt; y_pred = model.predict(dataset, n_jobs=4)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Evaluate predictions\n&gt;&gt;&gt; metric = model.evaluate(data.target, y_pred, r2_score)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Or get per-fold metrics\n&gt;&gt;&gt; metrics = model.evaluate(\n...     data.target, y_pred, r2_score, eval_mode=EvalMode.FOLD_WISE\n... )\n</code></pre>"},{"location":"reference/core/cvmodelcontainer/#factrainer.core.CvModelContainer-attributes","title":"Attributes","text":""},{"location":"reference/core/cvmodelcontainer/#factrainer.core.CvModelContainer.raw_model","title":"raw_model  <code>property</code>","text":"<pre><code>raw_model: RawModels[U]\n</code></pre> <p>Get the raw models from cross-validation.</p> <p>Returns:</p> Type Description <code>RawModels[U]</code> <p>The raw models as a RawModels object.</p>"},{"location":"reference/core/cvmodelcontainer/#factrainer.core.CvModelContainer.train_config","title":"train_config  <code>property</code> <code>writable</code>","text":"<pre><code>train_config: V\n</code></pre> <p>Get the training configuration.</p> <p>Returns:</p> Type Description <code>V</code> <p>The training configuration.</p>"},{"location":"reference/core/cvmodelcontainer/#factrainer.core.CvModelContainer.pred_config","title":"pred_config  <code>property</code> <code>writable</code>","text":"<pre><code>pred_config: W\n</code></pre> <p>Get the prediction configuration.</p> <p>Returns:</p> Type Description <code>W</code> <p>The prediction configuration.</p>"},{"location":"reference/core/cvmodelcontainer/#factrainer.core.CvModelContainer.cv_indices","title":"cv_indices  <code>property</code>","text":"<pre><code>cv_indices: SplittedDatasetsIndices\n</code></pre> <p>Get the cross-validation split indices after training.</p> <p>This property returns the cross-validation split indices that are stored in the instance after the <code>train</code> method is executed.</p> <p>Returns:</p> Type Description <code>SplittedDatasetsIndices</code> <p>The cross-validation split indices.</p>"},{"location":"reference/core/cvmodelcontainer/#factrainer.core.CvModelContainer.k_fold","title":"k_fold  <code>property</code>","text":"<pre><code>k_fold: _BaseKFold | SplittedDatasetsIndices\n</code></pre> <p>Get the cross-validation splitter.</p> <p>Returns:</p> Type Description <code>_BaseKFold or SplittedDatasetsIndices</code> <p>The cross-validation splitter.</p>"},{"location":"reference/core/cvmodelcontainer/#factrainer.core.CvModelContainer-functions","title":"Functions","text":""},{"location":"reference/core/cvmodelcontainer/#factrainer.core.CvModelContainer.train","title":"train","text":"<pre><code>train(train_dataset: T, n_jobs: int | None = None) -&gt; None\n</code></pre> <p>Train the model using cross-validation.</p> <p>This method trains the model using cross-validation, according to the specified cross-validation splitter. The trained models can be accessed through the <code>raw_model</code> property.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataset</code> <code>T</code> <p>The training dataset.</p> required <code>n_jobs</code> <code>int or None</code> <p>The number of jobs to run in parallel. If -1, all CPUs are used. If None, no parallel processing is used. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code>"},{"location":"reference/core/cvmodelcontainer/#factrainer.core.CvModelContainer.predict","title":"predict","text":"<pre><code>predict(\n    pred_dataset: T,\n    n_jobs: int | None = None,\n    mode: PredMode = OOF_PRED,\n) -&gt; Prediction\n</code></pre> <p>Make predictions using the trained models.</p> <p>This method makes predictions using the trained models. It supports two prediction modes:</p> <ul> <li> <p>Out-of-fold (OOF) predictions: Predictions for the training data using models trained on other folds.</p> </li> <li> <p>Averaging Ensemble predictions: Predictions using averaging ensemble of all trained models.This mode should ONLY be used for unseen data (test data), as using it on training data would lead to data leakage.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>pred_dataset</code> <code>T</code> <p>The dataset to make predictions for.</p> required <code>n_jobs</code> <code>int or None</code> <p>The number of jobs to run in parallel. If -1, all CPUs are used. If None, no parallel processing is used. Default is None.</p> <code>None</code> <code>mode</code> <code>PredMode</code> <p>The prediction mode. Can be either PredMode.OOF_PRED for out-of-fold predictions or PredMode.AVG_ENSEMBLE for averaging ensemble predictions. Default is PredMode.OOF_PRED.</p> <code>OOF_PRED</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>The predictions as a NumPy array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the prediction mode is invalid.</p>"},{"location":"reference/core/cvmodelcontainer/#factrainer.core.CvModelContainer.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    y_true: Target,\n    y_pred: Prediction,\n    eval_func: EvalFunc[X],\n    eval_mode: Literal[POOLING] = POOLING,\n) -&gt; X\n</code></pre><pre><code>evaluate(\n    y_true: Target,\n    y_pred: Prediction,\n    eval_func: EvalFunc[X],\n    eval_mode: Literal[FOLD_WISE],\n) -&gt; Sequence[X]\n</code></pre> <pre><code>evaluate(\n    y_true: Target,\n    y_pred: Prediction,\n    eval_func: EvalFunc[X],\n    eval_mode: EvalMode = POOLING,\n) -&gt; X | Sequence[X]\n</code></pre> <p>Evaluate the model's predictions against true values.</p> <p>This method evaluates predictions from cross-validation models. The predictions can be either out-of-fold (OOF) predictions or predictions on unseen data (held-out test set).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Target</code> <p>The true target values as a NumPy array.</p> required <code>y_pred</code> <code>Prediction</code> <p>The predicted values as a NumPy array. Must have the same shape as y_true. These can be: - Out-of-fold predictions from predict(mode=PredMode.OOF_PRED) - Predictions on unseen data from predict(mode=PredMode.AVG_ENSEMBLE)</p> required <code>eval_func</code> <code>EvalFunc[X]</code> <p>The evaluation function that takes (y_true, y_pred) and returns a metric. Common examples include sklearn.metrics functions like r2_score, mae, etc.</p> required <code>eval_mode</code> <code>EvalMode</code> <p>The evaluation mode: - EvalMode.POOLING: Compute a single metric across all predictions   (standard for both OOF evaluation and held-out test set evaluation) - EvalMode.FOLD_WISE: Compute metrics for each fold separately   (useful for analyzing per-fold performance in OOF predictions)</p> <code>EvalMode.POOLING</code> <p>Returns:</p> Type Description <code>X | Sequence[X]</code> <p>If eval_mode is POOLING, returns a single evaluation score of type X. If eval_mode is FOLD_WISE, returns a list of evaluation scores per fold.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If y_true or y_pred are not NumPy arrays.</p>"},{"location":"reference/core/evalmode/","title":"EvalMode","text":""},{"location":"reference/core/evalmode/#factrainer.core.EvalMode","title":"EvalMode","text":""},{"location":"reference/core/evalmode/#factrainer.core.EvalMode-attributes","title":"Attributes","text":""},{"location":"reference/core/evalmode/#factrainer.core.EvalMode.POOLING","title":"POOLING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>POOLING = auto()\n</code></pre>"},{"location":"reference/core/evalmode/#factrainer.core.EvalMode.FOLD_WISE","title":"FOLD_WISE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FOLD_WISE = auto()\n</code></pre>"},{"location":"reference/core/predmode/","title":"PredMode","text":""},{"location":"reference/core/predmode/#factrainer.core.PredMode","title":"PredMode","text":""},{"location":"reference/core/predmode/#factrainer.core.PredMode-attributes","title":"Attributes","text":""},{"location":"reference/core/predmode/#factrainer.core.PredMode.OOF_PRED","title":"OOF_PRED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OOF_PRED = auto()\n</code></pre>"},{"location":"reference/core/predmode/#factrainer.core.PredMode.AVG_ENSEMBLE","title":"AVG_ENSEMBLE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>AVG_ENSEMBLE = auto()\n</code></pre>"},{"location":"reference/core/singlemodelcontainer/","title":"SingleModelContainer","text":""},{"location":"reference/core/singlemodelcontainer/#factrainer.core.SingleModelContainer","title":"SingleModelContainer","text":"<pre><code>SingleModelContainer(\n    model_config: BaseMlModelConfig[T, U, V, W],\n)\n</code></pre> <p>Single model container for machine learning models.</p> <p>This class provides a container for a single machine learning model. It takes a model configuration and provides methods for training a model and making predictions.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>BaseMlModelConfig</code> <p>The model configuration, which includes the learner, predictor, training configuration, and prediction configuration.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lightgbm as lgb\n&gt;&gt;&gt; from sklearn.datasets import fetch_california_housing\n&gt;&gt;&gt; from sklearn.metrics import r2_score\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; from factrainer.core import SingleModelContainer\n&gt;&gt;&gt; from factrainer.lightgbm import LgbDataset, LgbModelConfig, LgbTrainConfig\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load data\n&gt;&gt;&gt; data = fetch_california_housing()\n&gt;&gt;&gt; train_X, test_X, train_y, test_y = train_test_split(\n...     data.data, data.target, test_size=0.2, random_state=1\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create datasets\n&gt;&gt;&gt; train_dataset = LgbDataset(dataset=lgb.Dataset(train_X, train_y))\n&gt;&gt;&gt; val_dataset = LgbDataset(dataset=lgb.Dataset(test_X, test_y))\n&gt;&gt;&gt; test_dataset = LgbDataset(dataset=lgb.Dataset(test_X, test_y))\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Configure model\n&gt;&gt;&gt; config = LgbModelConfig.create(\n...     train_config=LgbTrainConfig(\n...         params={\n...             \"objective\": \"regression\",\n...             \"seed\": 1,\n...             \"deterministic\": True,\n...             \"verbose\": -1,\n...         },\n...         callbacks=[lgb.early_stopping(100, verbose=False)],\n...     ),\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create and train model\n&gt;&gt;&gt; model = SingleModelContainer(config)\n&gt;&gt;&gt; model.train(train_dataset, val_dataset)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Make predictions\n&gt;&gt;&gt; y_pred = model.predict(test_dataset)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Evaluate predictions\n&gt;&gt;&gt; metric = model.evaluate(test_y, y_pred, r2_score)\n</code></pre>"},{"location":"reference/core/singlemodelcontainer/#factrainer.core.SingleModelContainer-attributes","title":"Attributes","text":""},{"location":"reference/core/singlemodelcontainer/#factrainer.core.SingleModelContainer.raw_model","title":"raw_model  <code>property</code>","text":"<pre><code>raw_model: U\n</code></pre> <p>Get the trained raw model.</p> <p>Returns:</p> Type Description <code>U</code> <p>The trained model as a RawModel object.</p>"},{"location":"reference/core/singlemodelcontainer/#factrainer.core.SingleModelContainer.train_config","title":"train_config  <code>property</code> <code>writable</code>","text":"<pre><code>train_config: V\n</code></pre> <p>Get the training configuration.</p> <p>Returns:</p> Type Description <code>V</code> <p>The training configuration.</p>"},{"location":"reference/core/singlemodelcontainer/#factrainer.core.SingleModelContainer.pred_config","title":"pred_config  <code>property</code> <code>writable</code>","text":"<pre><code>pred_config: W\n</code></pre> <p>Get the prediction configuration.</p> <p>Returns:</p> Type Description <code>W</code> <p>The prediction configuration.</p>"},{"location":"reference/core/singlemodelcontainer/#factrainer.core.SingleModelContainer-functions","title":"Functions","text":""},{"location":"reference/core/singlemodelcontainer/#factrainer.core.SingleModelContainer.train","title":"train","text":"<pre><code>train(\n    train_dataset: T, val_dataset: T | None = None\n) -&gt; None\n</code></pre> <p>Train the model. The trained model can be accessed through the <code>raw_model</code> property.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataset</code> <code>T</code> <p>The training dataset.</p> required <code>val_dataset</code> <code>T | None</code> <p>The validation dataset if needed.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code>"},{"location":"reference/core/singlemodelcontainer/#factrainer.core.SingleModelContainer.predict","title":"predict","text":"<pre><code>predict(pred_dataset: T) -&gt; Prediction\n</code></pre> <p>Make predictions using the trained model.</p> <p>Parameters:</p> Name Type Description Default <code>pred_dataset</code> <code>T</code> <p>The test dataset.</p> required <p>Returns:</p> Type Description <code>Prediction</code> <p>The predictions as a NumPy array.</p>"},{"location":"reference/core/singlemodelcontainer/#factrainer.core.SingleModelContainer.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    y_true: Target,\n    y_pred: Prediction,\n    eval_func: EvalFunc[X],\n) -&gt; X\n</code></pre> <p>Evaluate the model's predictions against true values.</p> <p>This method evaluates predictions from a single trained model, typically on a held-out test set or validation set.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Target</code> <p>The true target values as a NumPy array.</p> required <code>y_pred</code> <code>Prediction</code> <p>The predicted values as a NumPy array. Must have the same shape as y_true.</p> required <code>eval_func</code> <code>EvalFunc[X]</code> <p>The evaluation function that takes (y_true, y_pred) and returns a metric. Common examples include sklearn.metrics functions like r2_score, mae, etc.</p> required <p>Returns:</p> Type Description <code>X</code> <p>The evaluation score of type X, as returned by eval_func.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If y_true or y_pred are not NumPy arrays.</p>"},{"location":"reference/core/splitteddatasetsindices/","title":"SplittedDatasetsIndices","text":""},{"location":"reference/core/splitteddatasetsindices/#factrainer.core.SplittedDatasetsIndices","title":"SplittedDatasetsIndices","text":""},{"location":"reference/core/splitteddatasetsindices/#factrainer.core.SplittedDatasetsIndices-attributes","title":"Attributes","text":""},{"location":"reference/core/splitteddatasetsindices/#factrainer.core.SplittedDatasetsIndices.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"reference/core/splitteddatasetsindices/#factrainer.core.SplittedDatasetsIndices.train","title":"train  <code>instance-attribute</code>","text":"<pre><code>train: RowIndices\n</code></pre>"},{"location":"reference/core/splitteddatasetsindices/#factrainer.core.SplittedDatasetsIndices.val","title":"val  <code>instance-attribute</code>","text":"<pre><code>val: RowIndices | None\n</code></pre>"},{"location":"reference/core/splitteddatasetsindices/#factrainer.core.SplittedDatasetsIndices.test","title":"test  <code>instance-attribute</code>","text":"<pre><code>test: RowIndices\n</code></pre>"},{"location":"reference/lightgbm/","title":"factrainer.lightgbm","text":"<p>Optionally installable plugin package for LightGBM integration.</p>"},{"location":"reference/lightgbm/lgbdataset/","title":"LgbDataset","text":""},{"location":"reference/lightgbm/lgbdataset/#factrainer.lightgbm.LgbDataset","title":"LgbDataset","text":"<p>Wrapper for LightGBM Dataset providing factrainer-compatible interface.</p> <p>This class wraps a native <code>lgb.Dataset</code> to provide a consistent interface for cross-validation and data manipulation within the factrainer framework. This dataset is passed to the <code>train</code> and <code>predict</code> methods of <code>SingleModelContainer</code> and <code>CvModelContainer</code> when using LightGBM models.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>Dataset</code> <p>The underlying LightGBM Dataset instance containing features, labels, and optional metadata like weights, groups, and init scores.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import lightgbm as lgb\n&gt;&gt;&gt; from factrainer.lightgbm import LgbDataset\n&gt;&gt;&gt; # Create from numpy arrays\n&gt;&gt;&gt; X = np.random.randn(100, 10)\n&gt;&gt;&gt; y = np.random.randn(100)\n&gt;&gt;&gt; lgb_data = lgb.Dataset(X, label=y)\n&gt;&gt;&gt; dataset = LgbDataset(dataset=lgb_data)\n&gt;&gt;&gt; # Create with additional metadata\n&gt;&gt;&gt; weights = np.random.rand(100)\n&gt;&gt;&gt; lgb_data = lgb.Dataset(X, label=y, weight=weights)\n&gt;&gt;&gt; dataset = LgbDataset(dataset=lgb_data)\n</code></pre>"},{"location":"reference/lightgbm/lgbdataset/#factrainer.lightgbm.LgbDataset-attributes","title":"Attributes","text":""},{"location":"reference/lightgbm/lgbdataset/#factrainer.lightgbm.LgbDataset.dataset","title":"dataset  <code>instance-attribute</code>","text":"<pre><code>dataset: Dataset\n</code></pre>"},{"location":"reference/lightgbm/lgbmodel/","title":"LgbModel","text":""},{"location":"reference/lightgbm/lgbmodel/#factrainer.lightgbm.LgbModel","title":"LgbModel","text":"<p>Wrapper for trained LightGBM model.</p> <p>This class wraps a trained <code>lgb.Booster</code> instance to provide a consistent interface within the factrainer framework.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Booster</code> <p>The trained LightGBM Booster instance.</p>"},{"location":"reference/lightgbm/lgbmodel/#factrainer.lightgbm.LgbModel-attributes","title":"Attributes","text":""},{"location":"reference/lightgbm/lgbmodel/#factrainer.lightgbm.LgbModel.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Booster\n</code></pre>"},{"location":"reference/lightgbm/lgbmodelconfig/","title":"LgbModelConfig","text":""},{"location":"reference/lightgbm/lgbmodelconfig/#factrainer.lightgbm.LgbModelConfig","title":"LgbModelConfig","text":"<p>Configuration container for LightGBM models in the factrainer framework.</p> <p>This class encapsulates all necessary components for training and prediction with LightGBM models.</p> Warnings <p>Do not instantiate this class directly by calling <code>LgbModelConfig(...)</code>. Use the factory method <code>LgbModelConfig.create</code> instead.</p> <p>Attributes:</p> Name Type Description <code>learner</code> <code>LgbLearner</code> <p>Component responsible for training LightGBM models.</p> <code>predictor</code> <code>LgbPredictor</code> <p>Component responsible for making predictions with trained models.</p> <code>train_config</code> <code>LgbTrainConfig</code> <p>Configuration for training parameters (params, num_boost_round, etc.).</p> <code>pred_config</code> <code>LgbPredictConfig</code> <p>Configuration for prediction parameters (iteration range, output type, etc.).</p> See Also <p>LgbModelConfig.create : Factory method for creating configurations. LgbTrainConfig : Configuration for training parameters. LgbPredictConfig : Configuration for prediction parameters. SingleModelContainer : For training a single model. CvModelContainer : For cross-validation workflows.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from factrainer.lightgbm import LgbModelConfig, LgbTrainConfig\n&gt;&gt;&gt; # Create configuration with default prediction settings\n&gt;&gt;&gt; train_config = LgbTrainConfig(\n...     params={\"objective\": \"regression\", \"metric\": \"rmse\"}, num_boost_round=100\n... )\n&gt;&gt;&gt; model_config = LgbModelConfig.create(train_config)\n&gt;&gt;&gt; # Create configuration with custom prediction settings\n&gt;&gt;&gt; from factrainer.lightgbm import LgbPredictConfig\n&gt;&gt;&gt; pred_config = LgbPredictConfig(raw_score=True)\n&gt;&gt;&gt; model_config = LgbModelConfig.create(train_config, pred_config)\n</code></pre>"},{"location":"reference/lightgbm/lgbmodelconfig/#factrainer.lightgbm.LgbModelConfig-attributes","title":"Attributes","text":""},{"location":"reference/lightgbm/lgbmodelconfig/#factrainer.lightgbm.LgbModelConfig.learner","title":"learner  <code>instance-attribute</code>","text":"<pre><code>learner: LgbLearner\n</code></pre>"},{"location":"reference/lightgbm/lgbmodelconfig/#factrainer.lightgbm.LgbModelConfig.predictor","title":"predictor  <code>instance-attribute</code>","text":"<pre><code>predictor: LgbPredictor\n</code></pre>"},{"location":"reference/lightgbm/lgbmodelconfig/#factrainer.lightgbm.LgbModelConfig.train_config","title":"train_config  <code>instance-attribute</code>","text":"<pre><code>train_config: LgbTrainConfig\n</code></pre>"},{"location":"reference/lightgbm/lgbmodelconfig/#factrainer.lightgbm.LgbModelConfig.pred_config","title":"pred_config  <code>instance-attribute</code>","text":"<pre><code>pred_config: LgbPredictConfig\n</code></pre>"},{"location":"reference/lightgbm/lgbmodelconfig/#factrainer.lightgbm.LgbModelConfig-functions","title":"Functions","text":""},{"location":"reference/lightgbm/lgbmodelconfig/#factrainer.lightgbm.LgbModelConfig.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(\n    train_config: LgbTrainConfig,\n    pred_config: LgbPredictConfig | None = None,\n) -&gt; Self\n</code></pre> <p>Create a new LgbModelConfig instance.</p> <p>Parameters:</p> Name Type Description Default <code>train_config</code> <code>LgbTrainConfig</code> <p>Configuration for training parameters including LightGBM params, number of boosting rounds, callbacks, etc.</p> required <code>pred_config</code> <code>LgbPredictConfig | None</code> <p>Configuration for prediction parameters. If None, uses default LgbPredictConfig() which performs standard prediction.</p> <code>None</code> <p>Returns:</p> Type Description <code>LgbModelConfig</code> <p>A configuration instance ready for use with SingleModelContainer or CvModelContainer.</p>"},{"location":"reference/lightgbm/lgbpredictconfig/","title":"LgbPredictConfig","text":""},{"location":"reference/lightgbm/lgbpredictconfig/#factrainer.lightgbm.LgbPredictConfig","title":"LgbPredictConfig","text":"<p>Configuration for LightGBM prediction parameters.</p> <p>This class encapsulates all parameters that can be passed to the <code>predict()</code> method of a LightGBM Booster, except for the data parameter itself.</p> <p>Parameters:</p> Name Type Description Default <code>start_iteration</code> <code>int</code> <p>Start index of the iteration to predict. If &lt;= 0, starts from the first iteration.</p> <code>0</code> <code>num_iteration</code> <code>int | None</code> <p>Total number of iterations used in the prediction. - If None: if the best iteration exists and start_iteration &lt;= 0,   the best iteration is used; otherwise, all iterations from   start_iteration are used. - If &lt;= 0: all iterations from start_iteration are used (no limits).</p> <code>None</code> <code>raw_score</code> <code>bool</code> <p>Whether to predict raw scores. - If False: returns transformed scores (e.g., probabilities for   binary classification). - If True: returns raw scores before transformation (e.g., raw   log-odds for binary classification).</p> <code>False</code> <code>pred_leaf</code> <code>bool</code> <p>Whether to predict leaf indices. - If True: returns the index of the leaf that each sample ends up   in for each tree. Output shape is [n_samples, n_trees] or   [n_samples, n_trees * n_classes] for multiclass. - If False: returns predicted values.</p> <code>False</code> <code>pred_contrib</code> <code>bool</code> <p>Whether to predict feature contributions (SHAP values). - If True: returns feature contributions for each prediction,   including the base value (intercept) as the last column.   Output shape is [n_samples, n_features + 1] or   [n_samples, (n_features + 1) * n_classes] for multiclass. - If False: returns predicted values.</p> <code>False</code> <code>data_has_header</code> <code>bool</code> <p>Whether the data file has a header when data is provided as a file path. Only used when prediction data is a string path to a text file (CSV, TSV, or LibSVM).</p> <code>False</code> <code>validate_features</code> <code>bool</code> <p>Whether to validate that features in the prediction data match those used during training. Only applies when prediction data is a pandas DataFrame.</p> <code>False</code> See Also <p>lightgbm.Booster.predict : The underlying LightGBM prediction method.</p> Notes <ul> <li>Only one of <code>pred_leaf</code> and <code>pred_contrib</code> can be True at a time.</li> <li>When using custom objective functions, raw_score=False still returns   raw predictions since the transformation function is not known.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from factrainer.lightgbm import LgbPredictConfig\n&gt;&gt;&gt; # Standard prediction\n&gt;&gt;&gt; config = LgbPredictConfig()\n&gt;&gt;&gt; # Raw score prediction\n&gt;&gt;&gt; config = LgbPredictConfig(raw_score=True)\n&gt;&gt;&gt; # Get SHAP values\n&gt;&gt;&gt; config = LgbPredictConfig(pred_contrib=True)\n&gt;&gt;&gt; # Predict leaf indices\n&gt;&gt;&gt; config = LgbPredictConfig(pred_leaf=True)\n</code></pre>"},{"location":"reference/lightgbm/lgbpredictconfig/#factrainer.lightgbm.LgbPredictConfig-attributes","title":"Attributes","text":""},{"location":"reference/lightgbm/lgbpredictconfig/#factrainer.lightgbm.LgbPredictConfig.start_iteration","title":"start_iteration  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>start_iteration: int = 0\n</code></pre>"},{"location":"reference/lightgbm/lgbpredictconfig/#factrainer.lightgbm.LgbPredictConfig.num_iteration","title":"num_iteration  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_iteration: int | None = None\n</code></pre>"},{"location":"reference/lightgbm/lgbpredictconfig/#factrainer.lightgbm.LgbPredictConfig.raw_score","title":"raw_score  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_score: bool = False\n</code></pre>"},{"location":"reference/lightgbm/lgbpredictconfig/#factrainer.lightgbm.LgbPredictConfig.pred_leaf","title":"pred_leaf  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pred_leaf: bool = False\n</code></pre>"},{"location":"reference/lightgbm/lgbpredictconfig/#factrainer.lightgbm.LgbPredictConfig.pred_contrib","title":"pred_contrib  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pred_contrib: bool = False\n</code></pre>"},{"location":"reference/lightgbm/lgbpredictconfig/#factrainer.lightgbm.LgbPredictConfig.data_has_header","title":"data_has_header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_has_header: bool = False\n</code></pre>"},{"location":"reference/lightgbm/lgbpredictconfig/#factrainer.lightgbm.LgbPredictConfig.validate_features","title":"validate_features  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>validate_features: bool = False\n</code></pre>"},{"location":"reference/lightgbm/lgbtrainconfig/","title":"LgbTrainConfig","text":""},{"location":"reference/lightgbm/lgbtrainconfig/#factrainer.lightgbm.LgbTrainConfig","title":"LgbTrainConfig","text":"<p>Configuration for LightGBM training parameters.</p> <p>This class encapsulates all parameters that can be passed to <code>lgb.train()</code>, except for validation data-related parameters. Validation data handling is managed separately: <code>SingleModelContainer</code> decides whether to use validation data in its train method, while <code>CvModelContainer</code> automatically handles it during cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict[str, Any]</code> <p>Parameters for training.</p> required <code>num_boost_round</code> <code>int</code> <p>Number of boosting iterations.</p> <code>100</code> <code>valid_names</code> <code>list[str] | None</code> <p>Names of <code>valid_sets</code>.</p> <code>None</code> <code>feval</code> <code>callable or list of callable</code> <p>Customized evaluation function. Each evaluation function should accept two parameters: preds, eval_data, and return (eval_name, eval_result, is_higher_better) or list of such tuples.</p> required <code>init_model</code> <code>(str, Path, Booster or None)</code> <p>Filename of LightGBM model or Booster instance used for continue training.</p> required <code>keep_training_booster</code> <code>bool</code> <p>Whether the returned Booster will be used to keep training. If False, the returned value will be converted into _InnerPredictor before returning.</p> <code>False</code> <code>callbacks</code> <code>list of callable</code> <p>List of callback functions that are applied at each iteration.</p> required See Also <p>lightgbm.train : The underlying LightGBM training function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lightgbm as lgb\n&gt;&gt;&gt; from factrainer.lightgbm import LgbTrainConfig\n&gt;&gt;&gt; config = LgbTrainConfig(\n...     params={\n...         \"objective\": \"regression\",\n...         \"metric\": \"rmse\",\n...         \"boosting_type\": \"gbdt\",\n...         \"num_leaves\": 31,\n...         \"learning_rate\": 0.05,\n...     },\n...     num_boost_round=100,\n...     callbacks=[lgb.early_stopping(10), lgb.log_evaluation(50)],\n... )\n</code></pre>"},{"location":"reference/lightgbm/lgbtrainconfig/#factrainer.lightgbm.LgbTrainConfig-attributes","title":"Attributes","text":""},{"location":"reference/lightgbm/lgbtrainconfig/#factrainer.lightgbm.LgbTrainConfig.params","title":"params  <code>instance-attribute</code>","text":"<pre><code>params: dict[str, Any]\n</code></pre>"},{"location":"reference/lightgbm/lgbtrainconfig/#factrainer.lightgbm.LgbTrainConfig.num_boost_round","title":"num_boost_round  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_boost_round: int = 100\n</code></pre>"},{"location":"reference/lightgbm/lgbtrainconfig/#factrainer.lightgbm.LgbTrainConfig.valid_names","title":"valid_names  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>valid_names: list[str] | None = None\n</code></pre>"},{"location":"reference/lightgbm/lgbtrainconfig/#factrainer.lightgbm.LgbTrainConfig.feval","title":"feval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>feval: (\n    _LGBM_CustomMetricFunction\n    | list[_LGBM_CustomMetricFunction]\n    | None\n) = None\n</code></pre>"},{"location":"reference/lightgbm/lgbtrainconfig/#factrainer.lightgbm.LgbTrainConfig.init_model","title":"init_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>init_model: str | Path | Booster | None = None\n</code></pre>"},{"location":"reference/lightgbm/lgbtrainconfig/#factrainer.lightgbm.LgbTrainConfig.keep_training_booster","title":"keep_training_booster  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>keep_training_booster: bool = False\n</code></pre>"},{"location":"reference/lightgbm/lgbtrainconfig/#factrainer.lightgbm.LgbTrainConfig.callbacks","title":"callbacks  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>callbacks: list[Callable[..., Any]] | None = None\n</code></pre>"},{"location":"reference/sklearn/","title":"factrainer.sklearn","text":"<p>Optionally installable plugin package for Scikit-learn integration.</p>"},{"location":"reference/sklearn/sklearndataset/","title":"SklearnDataset","text":""},{"location":"reference/sklearn/sklearndataset/#factrainer.sklearn.SklearnDataset","title":"SklearnDataset","text":"<p>Wrapper for scikit-learn compatible data providing factrainer-compatible interface.</p> <p>This class wraps feature matrix and target vector to provide a consistent interface for cross-validation and data manipulation within the factrainer framework. It supports multiple data formats including numpy arrays, pandas DataFrames/Series, and polars DataFrames/Series. This dataset is passed to the <code>train</code> and <code>predict</code> methods of <code>SingleModelContainer</code> and <code>CvModelContainer</code> when using scikit-learn models.</p> <p>Attributes:</p> Name Type Description <code>X</code> <code>MatrixLike</code> <p>Feature matrix. Can be a numpy array, pandas DataFrame, or polars DataFrame.</p> <code>y</code> <code>VectorLike | None</code> <p>Target vector. Can be a numpy array, pandas Series, polars Series, or None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from factrainer.sklearn import SklearnDataset\n&gt;&gt;&gt; # Create from numpy arrays\n&gt;&gt;&gt; X = np.random.randn(100, 10)\n&gt;&gt;&gt; y = np.random.randn(100)\n&gt;&gt;&gt; dataset = SklearnDataset(X=X, y=y)\n&gt;&gt;&gt; # Create without target\n&gt;&gt;&gt; dataset = SklearnDataset(X=X)\n&gt;&gt;&gt; # Create from pandas\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(10)])\n&gt;&gt;&gt; target = pd.Series(y, name=\"target\")\n&gt;&gt;&gt; dataset = SklearnDataset(X=df, y=target)\n</code></pre>"},{"location":"reference/sklearn/sklearndataset/#factrainer.sklearn.SklearnDataset-attributes","title":"Attributes","text":""},{"location":"reference/sklearn/sklearndataset/#factrainer.sklearn.SklearnDataset.X","title":"X  <code>instance-attribute</code>","text":"<pre><code>X: MatrixLike\n</code></pre>"},{"location":"reference/sklearn/sklearndataset/#factrainer.sklearn.SklearnDataset.y","title":"y  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>y: VectorLike | None = None\n</code></pre>"},{"location":"reference/sklearn/sklearnmodel/","title":"SklearnModel","text":""},{"location":"reference/sklearn/sklearnmodel/#factrainer.sklearn.SklearnModel","title":"SklearnModel","text":"<p>Wrapper for trained scikit-learn model.</p> <p>This class wraps a trained scikit-learn estimator instance to provide a consistent interface within the factrainer framework.</p> <p>Attributes:</p> Name Type Description <code>estimator</code> <code>Predictable | ProbPredictable</code> <p>The trained scikit-learn estimator instance.</p>"},{"location":"reference/sklearn/sklearnmodel/#factrainer.sklearn.SklearnModel-attributes","title":"Attributes","text":""},{"location":"reference/sklearn/sklearnmodel/#factrainer.sklearn.SklearnModel.estimator","title":"estimator  <code>instance-attribute</code>","text":"<pre><code>estimator: Predictable | ProbPredictable\n</code></pre>"},{"location":"reference/sklearn/sklearnmodelconfig/","title":"SklearnModelConfig","text":""},{"location":"reference/sklearn/sklearnmodelconfig/#factrainer.sklearn.SklearnModelConfig","title":"SklearnModelConfig","text":"<p>Configuration container for scikit-learn models in the factrainer framework.</p> <p>This class encapsulates all necessary components for training and prediction with scikit-learn models.</p> Warnings <p>Do not instantiate this class directly by calling <code>SklearnModelConfig(...)</code>. Use the factory method <code>SklearnModelConfig.create</code> instead.</p> <p>Attributes:</p> Name Type Description <code>learner</code> <code>SklearnLearner</code> <p>Component responsible for training scikit-learn models.</p> <code>predictor</code> <code>SklearnPredictor</code> <p>Component responsible for making predictions with trained models.</p> <code>train_config</code> <code>SklearnTrainConfig</code> <p>Configuration for training parameters (estimator and fit kwargs).</p> <code>pred_config</code> <code>SklearnPredictConfig</code> <p>Configuration for prediction parameters (prediction method and kwargs).</p> See Also <p>SklearnModelConfig.create : Factory method for creating configurations. SklearnTrainConfig : Configuration for training parameters. SklearnPredictConfig : Configuration for prediction parameters. SingleModelContainer : For training a single model. CvModelContainer : For cross-validation workflows.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier\n&gt;&gt;&gt; from factrainer.sklearn import SklearnModelConfig, SklearnTrainConfig\n&gt;&gt;&gt; # Create configuration with default prediction settings\n&gt;&gt;&gt; estimator = RandomForestClassifier(n_estimators=10)\n&gt;&gt;&gt; train_config = SklearnTrainConfig(estimator=estimator)\n&gt;&gt;&gt; model_config = SklearnModelConfig.create(train_config)\n&gt;&gt;&gt; # Create configuration with custom prediction settings\n&gt;&gt;&gt; from factrainer.sklearn import SklearnPredictConfig, SklearnPredictMethod\n&gt;&gt;&gt; pred_config = SklearnPredictConfig(\n...     predict_method=SklearnPredictMethod.PREDICT_PROBA\n... )\n&gt;&gt;&gt; model_config = SklearnModelConfig.create(train_config, pred_config)\n</code></pre>"},{"location":"reference/sklearn/sklearnmodelconfig/#factrainer.sklearn.SklearnModelConfig-attributes","title":"Attributes","text":""},{"location":"reference/sklearn/sklearnmodelconfig/#factrainer.sklearn.SklearnModelConfig.learner","title":"learner  <code>instance-attribute</code>","text":"<pre><code>learner: SklearnLearner\n</code></pre>"},{"location":"reference/sklearn/sklearnmodelconfig/#factrainer.sklearn.SklearnModelConfig.predictor","title":"predictor  <code>instance-attribute</code>","text":"<pre><code>predictor: SklearnPredictor\n</code></pre>"},{"location":"reference/sklearn/sklearnmodelconfig/#factrainer.sklearn.SklearnModelConfig.train_config","title":"train_config  <code>instance-attribute</code>","text":"<pre><code>train_config: SklearnTrainConfig\n</code></pre>"},{"location":"reference/sklearn/sklearnmodelconfig/#factrainer.sklearn.SklearnModelConfig.pred_config","title":"pred_config  <code>instance-attribute</code>","text":"<pre><code>pred_config: SklearnPredictConfig\n</code></pre>"},{"location":"reference/sklearn/sklearnmodelconfig/#factrainer.sklearn.SklearnModelConfig-functions","title":"Functions","text":""},{"location":"reference/sklearn/sklearnmodelconfig/#factrainer.sklearn.SklearnModelConfig.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(\n    train_config: SklearnTrainConfig,\n    pred_config: SklearnPredictConfig | None = None,\n) -&gt; Self\n</code></pre> <p>Create a new SklearnModelConfig instance.</p> <p>Parameters:</p> Name Type Description Default <code>train_config</code> <code>SklearnTrainConfig</code> <p>Configuration for training parameters including the estimator and any additional fit keyword arguments.</p> required <code>pred_config</code> <code>SklearnPredictConfig | None</code> <p>Configuration for prediction parameters. If None, uses default SklearnPredictConfig() which automatically selects the prediction method.</p> <code>None</code> <p>Returns:</p> Type Description <code>SklearnModelConfig</code> <p>A configuration instance ready for use with SingleModelContainer or CvModelContainer.</p>"},{"location":"reference/sklearn/sklearnpredictconfig/","title":"SklearnPredictConfig","text":""},{"location":"reference/sklearn/sklearnpredictconfig/#factrainer.sklearn.SklearnPredictConfig","title":"SklearnPredictConfig","text":"<p>Configuration for scikit-learn prediction parameters.</p> <p>This class encapsulates parameters for prediction with scikit-learn models. Additional keyword arguments for the estimator's prediction methods can be passed as attributes of this configuration.</p> <p>Parameters:</p> Name Type Description Default <code>predict_method</code> <code>SklearnPredictMethod</code> <p>The prediction method to use:</p> <ul> <li>AUTO: Automatically selects predict_proba if available, otherwise predict.</li> <li>PREDICT: Uses the predict method (returns class labels or regression values).</li> <li>PREDICT_PROBA: Uses the predict_proba method (returns probability estimates).</li> </ul> <code>SklearnPredictMethod.AUTO</code> See Also <p>SklearnPredictMethod : Enum of available prediction methods.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from factrainer.sklearn import SklearnPredictConfig, SklearnPredictMethod\n&gt;&gt;&gt; # Default configuration (auto-selects method)\n&gt;&gt;&gt; config = SklearnPredictConfig()\n&gt;&gt;&gt; # Force using predict method\n&gt;&gt;&gt; config = SklearnPredictConfig(predict_method=SklearnPredictMethod.PREDICT)\n&gt;&gt;&gt; # Use predict_proba for probability estimates\n&gt;&gt;&gt; config = SklearnPredictConfig(predict_method=SklearnPredictMethod.PREDICT_PROBA)\n</code></pre>"},{"location":"reference/sklearn/sklearnpredictconfig/#factrainer.sklearn.SklearnPredictConfig-attributes","title":"Attributes","text":""},{"location":"reference/sklearn/sklearnpredictconfig/#factrainer.sklearn.SklearnPredictConfig.predict_method","title":"predict_method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>predict_method: SklearnPredictMethod = AUTO\n</code></pre>"},{"location":"reference/sklearn/sklearnpredictmethod/","title":"SklearnPredictMethod","text":""},{"location":"reference/sklearn/sklearnpredictmethod/#factrainer.sklearn.SklearnPredictMethod","title":"SklearnPredictMethod","text":"<p>Prediction method selection for scikit-learn models.</p> <p>Attributes:</p> Name Type Description <code>AUTO</code> <code>auto</code> <p>Automatically selects predict_proba if available, otherwise predict.</p> <code>PREDICT</code> <code>auto</code> <p>Uses the predict method (returns class labels or regression values).</p> <code>PREDICT_PROBA</code> <code>auto</code> <p>Uses the predict_proba method (returns probability estimates).</p>"},{"location":"reference/sklearn/sklearnpredictmethod/#factrainer.sklearn.SklearnPredictMethod-attributes","title":"Attributes","text":""},{"location":"reference/sklearn/sklearnpredictmethod/#factrainer.sklearn.SklearnPredictMethod.AUTO","title":"AUTO  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>AUTO = auto()\n</code></pre>"},{"location":"reference/sklearn/sklearnpredictmethod/#factrainer.sklearn.SklearnPredictMethod.PREDICT","title":"PREDICT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PREDICT = auto()\n</code></pre>"},{"location":"reference/sklearn/sklearnpredictmethod/#factrainer.sklearn.SklearnPredictMethod.PREDICT_PROBA","title":"PREDICT_PROBA  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PREDICT_PROBA = auto()\n</code></pre>"},{"location":"reference/sklearn/sklearntrainconfig/","title":"SklearnTrainConfig","text":""},{"location":"reference/sklearn/sklearntrainconfig/#factrainer.sklearn.SklearnTrainConfig","title":"SklearnTrainConfig","text":"<p>Configuration for scikit-learn training parameters.</p> <p>This class encapsulates the estimator to be trained. Additional keyword arguments for the estimator's <code>fit()</code> method can be passed as attributes of this configuration.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>Predictable | ProbPredictable</code> <p>A scikit-learn estimator instance that implements the fit method. Must also implement either predict or predict_proba method.</p> required See Also <p>sklearn.base.BaseEstimator : The base class for all scikit-learn estimators.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from factrainer.sklearn import SklearnTrainConfig\n&gt;&gt;&gt; # Basic configuration\n&gt;&gt;&gt; estimator = RandomForestRegressor(n_estimators=100, random_state=42)\n&gt;&gt;&gt; config = SklearnTrainConfig(estimator=estimator)\n&gt;&gt;&gt; # With additional fit keyword arguments\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import SGDClassifier\n&gt;&gt;&gt; sample_weights = np.array([1, 2, 1, 1, 2])\n&gt;&gt;&gt; estimator = SGDClassifier()\n&gt;&gt;&gt; config = SklearnTrainConfig(\n...     estimator=estimator,\n...     sample_weight=sample_weights,  # passed as kwargs to fit()\n... )\n</code></pre>"},{"location":"reference/sklearn/sklearntrainconfig/#factrainer.sklearn.SklearnTrainConfig-attributes","title":"Attributes","text":""},{"location":"reference/sklearn/sklearntrainconfig/#factrainer.sklearn.SklearnTrainConfig.estimator","title":"estimator  <code>instance-attribute</code>","text":"<pre><code>estimator: Predictable | ProbPredictable\n</code></pre>"},{"location":"reference/xgboost/","title":"factrainer.xgboost","text":"<p>Implementation Status</p> <p>The XGBoost integration is currently WIP. It will be added in a future release.</p>"}]}